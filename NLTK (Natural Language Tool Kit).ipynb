{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize as nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/purnachandra.dasari/anaconda3/lib/python3.6/site-packages/nltk/__init__.py'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BlanklineTokenizer',\n",
       " 'LineTokenizer',\n",
       " 'MWETokenizer',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'SExprTokenizer',\n",
       " 'SpaceTokenizer',\n",
       " 'StanfordSegmenter',\n",
       " 'TabTokenizer',\n",
       " 'TextTilingTokenizer',\n",
       " 'ToktokTokenizer',\n",
       " 'TreebankWordTokenizer',\n",
       " 'TweetTokenizer',\n",
       " 'WhitespaceTokenizer',\n",
       " 'WordPunctTokenizer',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_treebank_word_tokenizer',\n",
       " 'api',\n",
       " 'blankline_tokenize',\n",
       " 'casual',\n",
       " 'casual_tokenize',\n",
       " 'improved_close_quote_regex',\n",
       " 'improved_open_quote_regex',\n",
       " 'improved_punct_regex',\n",
       " 'line_tokenize',\n",
       " 'load',\n",
       " 'mwe',\n",
       " 'punkt',\n",
       " 're',\n",
       " 'regexp',\n",
       " 'regexp_span_tokenize',\n",
       " 'regexp_tokenize',\n",
       " 'repp',\n",
       " 'sent_tokenize',\n",
       " 'sexpr',\n",
       " 'sexpr_tokenize',\n",
       " 'simple',\n",
       " 'stanford_segmenter',\n",
       " 'string_span_tokenize',\n",
       " 'texttiling',\n",
       " 'toktok',\n",
       " 'treebank',\n",
       " 'util',\n",
       " 'word_tokenize',\n",
       " 'wordpunct_tokenize']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good muffins cost $3.88\n",
      "in NewYork & Washington DC.  Please buy me\n",
      "        ... two of them.\n",
      "\n",
      "Thanks.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize,wordpunct_tokenize\n",
    "# sent_tokenize -- will break in to sentences\n",
    "# word_tokenize -- will break into words\n",
    "s = '''Good muffins cost $3.88\\nin NewYork & Washington DC.  Please buy me\n",
    "        ... two of them.\\n\\nThanks.'''\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good muffins cost $3.88\\nin NewYork & Washington DC.', 'Please buy me\\n        ... two of them.', 'Thanks.']\n",
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'NewYork', '&', 'Washington', 'DC', '.', 'Please', 'buy', 'me', '...', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'NewYork', '&', 'Washington', 'DC', '.', 'Please', 'buy', 'me', '...', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(s)\n",
    "print(sentences) # a single sentence can be spread across multiple lines and can also have '\\n' character\n",
    "words = word_tokenize(s)\n",
    "print(words)\n",
    "wordpunct = wordpunct_tokenize(s)\n",
    "print(wordpunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "21\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences)) # It has ignored the empty lines (\\n\\n)\n",
    "print(len(words))  # It doesn't count the newlines \n",
    "print(len(wordpunct))  # It splits the words on punctuations too... See 3.88 it got split into 3, '.' and 88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function string_span_tokenize in module nltk.tokenize.util:\n",
      "\n",
      "string_span_tokenize(s, sep)\n",
      "    Return the offsets of the tokens in *s*, as a sequence of ``(start, end)``\n",
      "    tuples, by splitting the string at each occurrence of *sep*.\n",
      "    \n",
      "        >>> from nltk.tokenize.util import string_span_tokenize\n",
      "        >>> s = '''Good muffins cost $3.88\\nin New York.  Please buy me\n",
      "        ... two of them.\\n\\nThanks.'''\n",
      "        >>> list(string_span_tokenize(s, \" \"))\n",
      "        [(0, 4), (5, 12), (13, 17), (18, 26), (27, 30), (31, 36), (37, 37),\n",
      "        (38, 44), (45, 48), (49, 55), (56, 58), (59, 73)]\n",
      "    \n",
      "    :param s: the string to be tokenized\n",
      "    :type s: str\n",
      "    :param sep: the token separator\n",
      "    :type sep: str\n",
      "    :rtype: iter(tuple(int, int))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.tokenize.string_span_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 4), (5, 12), (13, 17), (18, 26), (27, 34), (35, 36), (37, 47), (48, 51), (52, 52), (53, 59), (60, 63), (64, 67), (68, 68), (69, 69), (70, 70), (71, 71), (72, 72), (73, 73), (74, 74), (75, 78), (79, 82), (83, 85), (86, 100)]\n",
      "*=**=**=**=**=**=**=**=**=**=*\n",
      "[(0, 20), (21, 50), (51, 75), (76, 76), (77, 77), (78, 90), (91, 99)]\n"
     ]
    }
   ],
   "source": [
    "string_positions = nltk.tokenize.string_span_tokenize(s,\" \")\n",
    "print(list(string_positions))\n",
    "print(\"*=*\"*10)\n",
    "string_positions = nltk.tokenize.string_span_tokenize(s,\".\")\n",
    "print(list(string_positions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Synonyms and Antonyms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('love.n.01'), Synset('love.n.02'), Synset('beloved.n.01'), Synset('love.n.04'), Synset('love.n.05'), Synset('sexual_love.n.02'), Synset('love.v.01'), Synset('love.v.02'), Synset('love.v.03'), Synset('sleep_together.v.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn=wordnet.synsets('love')\n",
    "print(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get pleasure from\n",
      "a strong positive emotion of regard and affection\n"
     ]
    }
   ],
   "source": [
    "print(syn[7].definition()) # 'love.v.01' definition\n",
    "print(syn[0].definition()) # 'love.n.01' definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This student sleeps with everyone in her dorm', 'Adam knew Eve', 'Were you ever intimate with this man?']\n"
     ]
    }
   ],
   "source": [
    "# You can also get the examples usage of each of the meaning of the words\n",
    "print(syn[9].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'love', 'passion', 'beloved', 'dear', 'dearest', 'honey', 'love', 'love', 'sexual_love', 'erotic_love', 'love', 'sexual_love', 'lovemaking', 'making_love', 'love', 'love_life', 'love', 'love', 'enjoy', 'love', 'sleep_together', 'roll_in_the_hay', 'love', 'make_out', 'make_love', 'sleep_with', 'get_laid', 'have_sex', 'know', 'do_it', 'be_intimate', 'have_intercourse', 'have_it_away', 'have_it_off', 'screw', 'fuck', 'jazz', 'eff', 'hump', 'lie_with', 'bed', 'have_a_go_at_it', 'bang', 'get_it_on', 'bonk']\n"
     ]
    }
   ],
   "source": [
    "# Suppose you want to find synonyms (similar words) related to 'love'. It's 3 step process:\n",
    "# Step 1: Get the Synsets\n",
    "# Step 2: For each synset iterate and get the lemmas\n",
    "# Step 3: For each lemma get the corresponding names\n",
    "love_similar_words = []\n",
    "\n",
    "for syn in wordnet.synsets('love'):  # Step 1\n",
    "    for l in syn.lemmas(): # Step 2\n",
    "        # There is no need for if condition because if there are no synonyms it would return []\n",
    "        love_similar_words.append(l.name()) # Step 3\n",
    "    \n",
    "print(love_similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'goodness', 'good', 'unregretful']\n"
     ]
    }
   ],
   "source": [
    "# Suppose you want to find antonyms (opposite words) related to 'bad'. It's 3 step process:\n",
    "# Step 1: Get the Synsets\n",
    "# Step 2: For each synset iterate and get the lemmas\n",
    "# Step 3: For each lemma get the corresponding antonyms\n",
    "bad_opposite_words = []\n",
    "\n",
    "for syn in wordnet.synsets('bad'):  # Step 1\n",
    "    for l in syn.lemmas(): # Step 2\n",
    "        if(l.antonyms()): # Not all words have opposites. Hence this if condition\n",
    "            bad_opposite_words.append(l.antonyms()[0].name()) # Step 3\n",
    "    \n",
    "print(bad_opposite_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming (identification of root word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write:write\n",
      "writer:writer\n",
      "writing:write\n",
      "writers:writer\n",
      "written:written\n",
      "wrote:wrote\n",
      "writt:writt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The Python stemming algorithms we have are often based on rules applying to suffix-stripping. \n",
    "The most common is the Porter-Stemmer\n",
    "\"\"\"\n",
    "from nltk.stem import PorterStemmer\n",
    "words=['write','writer','writing','writers','written','wrote','writt']\n",
    "ps = PorterStemmer()\n",
    "# Stemming individual words\n",
    "for w in words:\n",
    "    print(w+':'+ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am enjoying writing this tutorial; I love to write and I have written 266 words so far. I wrote more than you did; I am a writer.\n",
      "I:I\n",
      "am:am\n",
      "enjoying:enjoy\n",
      "writing:write\n",
      "this:thi\n",
      "tutorial:tutori\n",
      ";:;\n",
      "I:I\n",
      "love:love\n",
      "to:to\n",
      "write:write\n",
      "and:and\n",
      "I:I\n",
      "have:have\n",
      "written:written\n",
      "266:266\n",
      "words:word\n",
      "so:so\n",
      "far:far\n",
      ".:.\n",
      "I:I\n",
      "wrote:wrote\n",
      "more:more\n",
      "than:than\n",
      "you:you\n",
      "did:did\n",
      ";:;\n",
      "I:I\n",
      "am:am\n",
      "a:a\n",
      "writer:writer\n",
      ".:.\n"
     ]
    }
   ],
   "source": [
    "sentence='I am enjoying writing this tutorial; I love to write and I have written 266 words so far. I wrote more than you did; I am a writer.'\n",
    "print(sentence)\n",
    "# You can pass the sentence..by tokenizing in to words and then calling stemmer\n",
    "sentence_words = word_tokenize(sentence)\n",
    "for w in sentence_words:\n",
    "    print(w+':'+ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer (group related words together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs: dog\n",
      "bowlers: bowler\n",
      "child: child\n",
      "kids: kid\n",
      "children: child\n",
      "babies: baby\n",
      "lover: lover\n",
      "lovers: lover\n",
      "loving: loving\n",
      "loved: loved\n",
      "tobeloved: tobeloved\n",
      "love: love\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lemmatizer, groups together inflected forms of a word. It links words with similar meanings to \n",
    "one word and maps various words onto one root.\n",
    "\"\"\"\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm_words = ['dogs','bowlers','child','kids','children','babies','lover','lovers','loving','loved','tobeloved','love']\n",
    "wnl = WordNetLemmatizer()\n",
    "for l in lemm_words:\n",
    "    print(l+': '+wnl.lemmatize(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between Lemmatization & Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lemmatization is the process of converting a word to its base form. \\nThe difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Lemmatization is the process of converting a word to its base form. \n",
    "The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Chapter-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This says \"from NLTK\\'s book module, load all items.\" \\nThe book module contains all the data you will need as you read this chapter. \\nAfter printing a welcome message, it loads the text of several books (this will take a few seconds).'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "'''This says \"from NLTK's book module, load all items.\" \n",
    "The book module contains all the data you will need as you read this chapter. \n",
    "After printing a welcome message, it loads the text of several books (this will take a few seconds).'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: Moby Dick by Herman Melville 1851>\n"
     ]
    }
   ],
   "source": [
    "print(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 11 of 11 matches:\n",
      "ong the former , one was of a most monstrous size . ... This came towards us , \n",
      "ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\n",
      "ll over with a heathenish array of monstrous clubs and spears . Some were thick\n",
      "d as you gazed , and wondered what monstrous cannibal and savage could ever hav\n",
      "that has survived the flood ; most monstrous and most mountainous ! That Himmal\n",
      "they might scout at Moby Dick as a monstrous fable , or still worse and more de\n",
      "th of Radney .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l\n",
      "ing Scenes . In connexion with the monstrous pictures of whales , I am strongly\n",
      "ere to enter upon those still more monstrous stories of them which are to be fo\n",
      "ght have been rummaged out of this monstrous cabinet there is no telling . But \n",
      "of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n"
     ]
    }
   ],
   "source": [
    "# A concordance view shows us every occurrence of a given word, together with some context\n",
    "text1.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whale will fear storm sight story place reason ashes bed rest string\n",
      "model lowering school heart body see was world\n"
     ]
    }
   ],
   "source": [
    "'''A concordance permits us to see words in context. For example, we saw that monstrous occurred in contexts such as the ___ pictures and a ___ size .\n",
    "What other words appear in a similar range of contexts? We can find out by appending the term similar to the name of the text in question, \n",
    "then inserting the relevant word in parentheses:'''\n",
    "\n",
    "text1.similar(\"care\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very so exceedingly heartily a as good great extremely remarkably\n",
      "sweet vast amazingly\n"
     ]
    }
   ],
   "source": [
    "text2.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38095238095238093\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('cactus.n.01')\n",
    "\n",
    "wc = w1.wup_similarity(w2)\n",
    "print(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CommonContexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The term common_contexts allows us to examine just the contexts that are shared by two or more words, \n",
    "such as monstrous and very. We have to enclose these words by square brackets as well as parentheses, \n",
    "and separate them with a comma:'''\n",
    "\n",
    "text2.common_contexts(['monstrous','very'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dispersion Plot (cover this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text3) # To find out the length of the text use this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set & Sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set(text3)) # Here the set is used to collapse the duplicates in text and sorted to bring in the order for all vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Monty'\n",
    "name[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrequencyDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdist1 = FreqDist(text1)\n",
    "freqdist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = gutenberg.raw(\"austen-sense.txt\")\n",
    "sentences = sent_tokenize(sample)\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
